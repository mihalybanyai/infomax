{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "699b5bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from infomax_class import distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f6a2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  2.  3.2 4. ]\n"
     ]
    }
   ],
   "source": [
    "class variable_length_generative_model(ABC):\n",
    "\n",
    "    def __init__(self, param_range, n_possible_obs):\n",
    "        # TODO generalise to continuous observations\n",
    "        self.prior = distribution(param_range)\n",
    "        self.n_possible_obs = n_possible_obs\n",
    "\n",
    "        self.possible_sequences = {}  # TODO do this with a storage struct\n",
    "        self.kl_components = {}\n",
    "        # TODO load storage\n",
    "\n",
    "    def set_prior(self, prob_densities):\n",
    "        self.prior.set_probs(prob_densities)\n",
    "\n",
    "    def possible_observation_sequences(self, N_distr):\n",
    "        if N_distr.to_array() in self.possible_sequences.keys():\n",
    "            return self.possible_sequences[N_distr.to_array()]\n",
    "        else:\n",
    "            # TODO do this for variable Ns\n",
    "            sequences = [list(i) for i in itertools.product(list(range(self.n_possible_obs)), repeat=N)]\n",
    "            self.possible_sequences[N_distr.to_array()] = sequences\n",
    "            return sequences\n",
    "\n",
    "    @abstractmethod\n",
    "    def observation_likelihood(self, observation, param_value):\n",
    "        # p(x | \\theta)\n",
    "        pass\n",
    "\n",
    "    def observation_marginal(self, observation):\n",
    "        # p(x) = \\sum_\\theta p(x | \\theta) p(\\theta)\n",
    "        all_like = [self.observation_likelihood(observation, self.prior.eval_points[i]) * self.prior.prob_densities[i] for i in range(self.prior.eval_num)]\n",
    "        return sum(all_like)\n",
    "\n",
    "    def sequence_likelihood(self, observations, param_value):\n",
    "        # p(X | theta) = \\prod_x p(x | \\theta)\n",
    "        # the probability of observing every observation in the sequence, given the parameter\n",
    "        # product of the individual likelihoods, as observations are i.i.d.\n",
    "        return np.prod(np.array([self.observation_likelihood(o, param_value) for o in observations]))\n",
    "\n",
    "    def sequence_marginal(self, observations):\n",
    "        # p(X) = \\sum_\\theta p(X | \\theta) p(\\theta)\n",
    "        # probability of observing the sequence given the entire prior distribution of the parameter instead of one specific value\n",
    "        all_like = [self.sequence_likelihood(observations, self.prior.eval_points[i]) * self.prior.prob_densities[i] for i in range(self.prior.eval_num)]\n",
    "        return sum(all_like)\n",
    "    \n",
    "    def _likelihoods_for_all_param(self, observation):\n",
    "        return np.array([self.observation_likelihood(observation, self.prior.eval_points[i]) for i in range(self.prior.eval_num)])\n",
    "\n",
    "    def _observation_prob_ratios(self, observation):\n",
    "        # p(x | \\theta) / \\sum_\\theta p(x | \\theta) p(\\theta)\n",
    "        likelihoods = self._likelihoods_for_all_param(observation)\n",
    "        marginal = np.sum(likelihoods * self.prior.prob_densities)\n",
    "        #print(\"likes\", likelihoods, \"marg\", marginal)\n",
    "        return likelihoods / marginal\n",
    "    \n",
    "    def _predictive_distr(self, act_prior):\n",
    "        possible_observations = list(range(self.n_possible_obs))\n",
    "        predictive_probs = [sum([self.observation_likelihood(o, act_prior.eval_points[th]) * act_prior.prob_densities[th] for th in range(act_prior.eval_num)]) for o in possible_observations]\n",
    "        return distribution(possible_observations, predictive_probs)\n",
    "\n",
    "    def KL_divergences(self, N, posterior=None, M=0, clip=1e-6):\n",
    "        # we reuse computation in this house\n",
    "        storage_key = tuple(list(self.prior.prob_densities) + [N])\n",
    "        if not storage_key in self.kl_components.keys():\n",
    "            all_sequences = self.possible_observation_sequences(N)\n",
    "            all_sequence_likelihoods = np.zeros((self.prior.eval_num, len(all_sequences)))  # p(X | \\theta) \\forall X, \\theta\n",
    "            for i_theta, p_theta in enumerate(self.prior.prob_densities):\n",
    "                if p_theta == 0: continue\n",
    "                for i_obs, obs in enumerate(all_sequences):\n",
    "                    all_sequence_likelihoods[i_theta, i_obs] = self.sequence_likelihood(obs, self.prior.eval_points[i_theta])\n",
    "\n",
    "            like_prior = all_sequence_likelihoods.transpose() * self.prior.prob_densities  # p(X | \\theta) p(\\theta) \\forall X, \\theta\n",
    "            sequence_marginals = np.sum(like_prior.transpose(), axis=0)  # p(X) \\forall X\n",
    "\n",
    "            log_sequence_marginals = np.log(sequence_marginals, out=np.zeros_like(sequence_marginals, dtype=np.float64), where=(sequence_marginals!=0))\n",
    "            log_sequence_likelihoods = np.log(all_sequence_likelihoods, out=np.zeros_like(all_sequence_likelihoods, dtype=np.float64), where=(all_sequence_likelihoods!=0))\n",
    "            logdiff = log_sequence_likelihoods - log_sequence_marginals\n",
    "\n",
    "            kl_components = all_sequence_likelihoods.transpose() * (logdiff.transpose())  # p(X | \\theta) [\\log p(X | \\theta) - \\log p(\\theta)] \\forall X, \\theta\n",
    "            self.kl_components[storage_key] = kl_components\n",
    "        future_KLs = np.nansum(self.kl_components[storage_key], axis=0)  # KL[p(X | \\theta) || p(X)] \\forall \\theta\n",
    "        \n",
    "        if posterior is None or M==0:\n",
    "            return future_KLs\n",
    "        else:\n",
    "            # take M samples from the posterior-predictive distribution \\sum_\\theta p(x | \\theta) p(\\theta | X_old)\n",
    "            # TODO reuse this part as well\n",
    "            samples = self._predictive_distr(posterior).sample(M)\n",
    "            sample_likelihoods = np.array([self._likelihoods_for_all_param(s) for s in samples])  # M x theta_res\n",
    "            sample_prob_ratios = np.array([self._observation_prob_ratios(s) for s in samples])  # M x theta_res\n",
    "            log_sample_prob_ratios = np.log(sample_prob_ratios, out=np.zeros_like(sample_prob_ratios, dtype=np.float64), where=(sample_prob_ratios!=0))\n",
    "            #print(sample_prob_ratios)\n",
    "            # we average over the samples\n",
    "            #KLs = np.sum(sample_prob_ratios * (log_sample_prob_ratios + future_KLs), axis=0) / M\n",
    "            # TODO why does this work way better than the one in the formulas???\n",
    "            KLs = np.sum(sample_likelihoods * (log_sample_prob_ratios + future_KLs), axis=0) / M\n",
    "            # the sample-based apprixmation can come back negative, so we clip\n",
    "            return np.maximum(KLs, 0.) \n",
    "\n",
    "    def mutual_information(self, N, posterior=None, M=0, clip=1e-6):\n",
    "        # TODO implement the version with sampling\n",
    "        return np.nansum(self.prior.prob_densities * self.KL_divergences(N, posterior=posterior, M=M, clip=clip))\n",
    "\n",
    "    def blahut_arimoto_prior(self, N, prior_res, n_step, posterior=None, M=0, min_delta=0, plot=False):\n",
    "        self.set_prior(np.ones(prior_res) / prior_res)\n",
    "        MIs = [self.mutual_information(N, posterior=posterior, M=M)]\n",
    "\n",
    "        for step in range(n_step):\n",
    "            act_kl = self.KL_divergences(N, posterior=posterior, M=M)\n",
    "            exp_kl = np.exp(act_kl)\n",
    "            unnorm_new_p = exp_kl * self.prior.prob_densities\n",
    "            #print(act_kl, unnorm_new_p)\n",
    "            self.set_prior(unnorm_new_p / np.sum(unnorm_new_p))\n",
    "            MIs.append(self.mutual_information(N, posterior=posterior, M=M))\n",
    "            if posterior is None or M==0:\n",
    "                if MIs[-1] - MIs[-2] <= min_delta:\n",
    "                    break\n",
    "\n",
    "        if plot:\n",
    "            plt.subplot(1, 2, 1)\n",
    "            self.prior.plot()\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(MIs)\n",
    "\n",
    "    def posterior(self, observations):\n",
    "        if self.prior.prob_densities is None:\n",
    "            raise RuntimeError(\"Prior not set, cannot calculate posterior.\")\n",
    "        unnorm_post = np.array([self.sequence_likelihood(observations, self.prior.eval_points[th]) * self.prior.prob_densities[th] for th in range(self.prior.eval_num)])\n",
    "        post = unnorm_post / np.sum(unnorm_post)\n",
    "        return distribution(self.prior.range, post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1400fefe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
